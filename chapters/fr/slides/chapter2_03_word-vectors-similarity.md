---
type: slides
---

# Vecteurs de mots et similarit√© s√©mantique

Notes : Dans cette le√ßon, tu vas apprendre √† utiliser spaCy pour pr√©dire √† quel
point des documents, des spans ou des tokens sont similaires les uns avec les
autres.

Tu vas aussi apprendre √† utiliser les vecteurs de mots et √† les exploiter dans
ton application de NLP.

---

# Comparaison de similarit√© s√©mantique

- `spaCy` peut comparer deux objets et pr√©dire leur similarit√©
- `Doc.similarity()`, `Span.similarity()` et `Token.similarity()`
- Accepte un autre objet et retourne un score de similarit√© (de `0` √† `1`)
- **Important :** n√©cessite un mod√®le qui inclut les vecteurs de mots, par
  exemple:
  - ‚úÖ `en_core_web_md` (mod√®le moyen)
  - ‚úÖ `en_core_web_lg` (grand mod√®le)
  - üö´ **PAS** `en_core_web_sm` (petit mod√®le)

Notes : spaCy peut comparer deux objets et pr√©dire √† quel point ils sont
similaires ‚Äì par exemple, documents, spans ou simples tokens.

Les objets `Doc`, `Token` et `Span` poss√®dent une m√©thode `.similarity` qui
prend en argument un autre objet et retourne un nombre d√©cimal entre 0 et 1,
indiquant dans quelle mesure ils sont similaires.

Un point tr√®s important : Pour pouvoir utiliser la similarit√©, tu dois utiliser
un mod√®le spaCy plus grand qui inclut les vecteurs de mots.

Par exemple, le mod√®le anglais moyen ou grand ‚Äì mais _pas_ le petit. Donc si tu
veux utiliser les vecteurs, choisis toujours un mod√®le qui se termine par "md"
ou par "lg". Tu trouveras de plus amples informations dans la
[documentation des mod√®les](https://spacy.io/models).

---

# Exemples de similarit√© (1)

```python
# Charge un plus grand mod√®le avec les vecteurs
nlp = spacy.load("en_core_web_md")

# Compare deux documents
doc1 = nlp("I like fast food")
doc2 = nlp("I like pizza")
print(doc1.similarity(doc2))
```

```out
0.8627204117787385
```

```python
# Compare deux tokens
doc = nlp("I like pizza and pasta")
token1 = doc[2]
token2 = doc[4]
print(token1.similarity(token2))
```

```out
0.7369546
```

Notes : Voici un exemple. Disons que nous voulons savoir si deux documents sont
similaires.

D'abord, nous chargeons le mod√®le anglais de taille moyenne, "en_core_web_md".

Nous pouvons ensuite cr√©er deux objets doc et utiliser la m√©thode `similarity`
du premier doc pour le comparer au second.

Ici, la pr√©diction est un score plut√¥t √©lev√© de similarit√© de 0,86 pour "I like
fast food" et "I like pizza".

Cela fonctionne aussi avec les tokens.

Selon les vecteurs de mots, les tokens "pizza" et "pasta" sont relativement
similaires, et obtiennent un score de 0,7.

---

# Exemples de similarit√© (2)

```python
# Compare un document avec un token
doc = nlp("I like pizza")
token = nlp("soap")[0]

print(doc.similarity(token))
```

```out
0.32531983166759537
```

```python
# Compare un span avec un document
span = nlp("I like pizza and pasta")[2:5]
doc = nlp("McDonalds sells burgers")

print(span.similarity(doc))
```

```out
0.619909235817623
```

Notes : Tu peux aussi utiliser les m√©thodes `similarity` pour comparer des
objets de types diff√©rents.

Par exemple, un document et un token.

Ici, le score de similarit√© est assez bas et les deux objets sont consid√©r√©s
assez peu similaires.

Voici un autre exemple comparant un span ‚Äì "pizza and pasta" ‚Äì √† un document
relatif √† McDonalds.

Le score retourn√© ici est 0,61, donc il y a une forme de similarit√©.

---

# Comment spaCy pr√©dit la similarit√© ?

- La similarit√© est d√©termin√©e en utilisant des **vecteurs de mots**
- Des repr√©sentations multi-dimensionnelles de la signification des mots
- G√©n√©r√©es avec un algorithme comme
  [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) et beaucoup de textes
- Peuvent √™tre ajout√©s aux mod√®les statistiques de spaCy
- Par d√©faut : similarit√© cosinus, mais peut √™tre modifi√©e
- Les vecteurs des `Doc` et `Span` sont par d√©faut la moyenne des vecteurs des
  tokens
- Les phrases courtes sont meilleures que les longs documents comportant de
  nombreux mots non pertinents

Notes : Mais comment spaCy fait-il √ßa sous le capot ?

La similarit√© est d√©termin√©e en utilisant des vecteurs de mots, des
repr√©sentations multi-dimensionnelles de la signification des mots.

Tu as peut-√™tre entendu parler de Word2Vec, c'est un algorithme qui est souvent
utilis√© pour entrainer des vecteurs de mots √† partir de textes bruts.

Les vecteurs peuvent √™tre ajout√©s aux mod√®les statistiques de spaCy.

Par d√©faut, la similarit√© retourn√©e par spaCy est la similarit√© cosinus entre
deux vecteurs - mais cela peut √™tre modifi√© si n√©cessaire.

Les vecteurs des objets compos√©s de plusieurs tokens, comme le `Doc` et le
`Span`, sont par d√©faut la moyenne des vecteurs de leurs tokens.

C'est aussi pour cela que tu obtiens g√©n√©ralement de meilleurs r√©sultats avec
des phrases courtes qui comportent moins de mots non pertinents.

---

# Les vecteurs de mots dans spaCy

```python
# Charge un plus grand mod√®le avec des vecteurs
nlp = spacy.load("en_core_web_md")

doc = nlp("I have a banana")
# Acc√®de au vecteur via l'attribut token.vector
print(doc[3].vector)
```

```out
 [2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,
  3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,
 -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,
  5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,
 -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,
  1.73800007e-01,   1.67720005e-01,   8.39839995e-01,
  5.51070012e-02,   1.05470002e-01,   3.78719985e-01,
  2.42750004e-01,   1.47449998e-02,   5.59509993e-01,
  1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,
 -4.00279984e-02,   9.59490016e-02,  -5.06900012e-01,
 -8.53179991e-02,   1.79800004e-01,   3.38669986e-01,
  ...
```

Notes : Pour te donner une id√©e, voici un exemple montrant √† quoi ressemblent
ces vecteurs.

D'abord, nous chargeons √† nouveau le mod√®le moyen, qui comporte des vecteurs de
mots.

Ensuite, nous pouvons traiter un texte et chercher le vecteur d'un token en
utilisant l'attribut `.vector`.

Le r√©sultat est un vecteur √† 300 dimensions du mot "banana".

---

# La similarit√© depend du contexte d'application

- Utile pour de nombreuses applications : syst√®mes de recommandations, rep√©rage
  de doublons etc.
- Il n'y a pas de d√©finition objective de "similarit√©"
- Cela d√©pend du contexte et des besoins de l'application

```python
doc1 = nlp("I like cats")
doc2 = nlp("I hate cats")

print(doc1.similarity(doc2))
```

```out
0.9501447503553421
```

Notes : Pr√©dire la similarit√© peut s'av√©rer utile pour toutes sortes
d'applications. Par exemple, pour recommander √† un utilisateur des textes
similaires √† ceux qu'il a lus. C'est aussi utile pour rep√©rer du contenu en
doublon, comme des posts sur une plateforme en ligne.

Toutefois, il est important de retenir qu'il n'y a pas de d√©finition objective
de ce qui est similaire et de ce qui ne l'est pas. Cela d√©pend toujours du
contexte et des besoins de ton application.

Voici un exemple : les vecteurs de mots de spaCy attribuent par d√©faut un score
√©lev√© de similarit√© entre "I like cats" et "I hate cats". Cela parait logique,
car les deux textes expriment des sentiments √† propos des chats. Mais dans un
contexte d'application diff√©rent, tu pourrais vouloir consid√©rer les phrases
comme √©tant tr√®s _dissemblables_, parce qu'elles expriment des sentiments
oppos√©s.

---

# Pratiquons !

Notes : Maintenant c'est √† ton tour. Essayons quelques vecteurs de mots de spaCy
et utilisons-les pour pr√©dire des similarit√©s.
