---
type: slides
---

# Vetores de palavras e similaridades sem√¢nticas

Notes: Nesta li√ß√£o voc√™ aprender√° a usar a biblioteca spaCy para prever o qu√£o dois
documentos, parti√ß√µes ou tokens s√£o similares entre si.

Voc√™ tamb√©m aprender√° a usar vetores de palavras e como tirar vantagem
de seu uso em aplica√ß√µes de PLN.

---

# Comparando similaridades sem√¢nticas

- A biblioteca `spaCy` pode comparar dois objetos e prever a sua similaridade.
- `Doc.similarity()`, `Span.similarity()` e `Token.similarity()`
- Recebem outro objeto e retornam um score de similaridade ( entre `0` e `1` )
- **Importante:** √© necess√°rio incluir um fluxo (pipeline) de processamento que tenha vetores de palavras incluso,
como por exemplo:
  - ‚úÖ `en_core_web_md` ou `pt_core_news_md` ( tamanho m√©dio )
  - ‚úÖ `en_core_web_lg` ou `pt_core_news_lg` ( tamanho grande )
  - üö´ **N√ÉO USE** `en_core_web_sm` ou `pt_core_news_sm` ( tamanho pequeno )

Notes: A spaCy consegue comparar dois objetos e prever o qu√£o similares eles s√£o
entre si. Os objetos podem ser documentos, parti√ß√µes ou tokens.

Os objetos `Doc`, `Token` e `Span` possuem o m√©todo `.similarity` que recebe
outro objeto e retorna um n√∫mero de ponto flutuante entre 0 e 1 indicando o
qu√£o similares estes objetos s√£o entre si.

Um detalhe importante: para poder usar a similaridade, voc√™ necessita usar um
fluxo (pipeline) de processamento maior que inclua a representa√ß√£o das palavras em vetores (word vectors).

Voc√™ pode usar o fluxo (pipeline) de processamento m√©dio ou grande da l√≠ngua inglesa, mas _n√£o_ o modelo pequeno.
Se voc√™ desejar usar os vetores, sempre use um fluxo (pipeline) de processamento que termine com os caracteres
"md" ou "lg". Para mais detalhes, visite a [documenta√ß√£o dos modelos](https://spacy.io/models).

---

# Exemplos de similaridades (1)

```python
# Carregar o fluxo (pipeline) de processamento maior com os vetores
nlp = spacy.load("en_core_web_md")

# Comparar dois documentos
doc1 = nlp("I like fast food")
doc2 = nlp("I like pizza")
print(doc1.similarity(doc2))
```

```out
0.8627204117787385
```

```python
# Comparar dois tokens
doc = nlp("I like pizza and pasta")
token1 = doc[2]
token2 = doc[4]
print(token1.similarity(token2))
```

```out
0.7369546
```

Notes: Aqui est√° um exemplo: vamos supor que voc√™ deseja saber se dois documentos
s√£o similares.

Inicialmente carregamos o modelo m√©dio da l√≠ngua inglesa : "en_core_web_md".

Em seguida podemos criar dois objetos o usar o m√©todo  `similarity` do primeiro
documento, comparando com o segundo.

Neste caso, encontramos uma similaridade razoavelmente alta entre "I like fast food"
e "I like pizza".

O mesmo pode ser feito para tokens.

De acordo com os vetores de palavras, os tokens "pizza" e "pasta" s√£o relativamente
similares e receberam um score de 0.7.

---

# Exemplos de similaridades (2)

```python
# Comparar um documento com um token
doc = nlp("I like pizza")
token = nlp("soap")[0]

print(doc.similarity(token))
```

```out
0.32531983166759537
```

```python
# Comparar uma parti√ß√£o com um documento
span = nlp("I like pizza and pasta")[2:5]
doc = nlp("McDonalds sells burgers")

print(span.similarity(doc))
```

```out
0.619909235817623
```

Notes: Voc√™ tamb√©m pode usar o m√©todo `similarity` para comparar tipos de
objetos diferentes.

Por exemplo: um documento e um token.

No primeiro exemplo, o score de similaridade √© bem baixo e podemos considerar que esses
objetos n√£o s√£o similares.

No outro exemplo comparamos uma parti√ß√£o "pizza and pasta" com um documento
sobre McDonalds.

O score foi 0.61, que significa que s√£o um pouco similares.

---

# Como a spaCy prev√™ similaridades?

- A similaridade √© determinada usando os **vetores de palavras**
- Vetores s√£o representa√ß√µes multi dimensionais das palavras
- S√£o gerados utilizando algoritmos similares a 
  [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) e uma enorme quantidade de textos.
- Podem ser adicionados aos fluxos (pipelines) de processamento da spaCy.
- Algoritmo padr√£o: similaridade por cosseno, mas pode ser alterado
- Os vetores de `Doc` e `Span` s√£o a m√©dia dos vetores de seus tokens.
- Frases curtas s√£o melhores que grandes documentos com palavras irrelevantes.

Notes: Mas como a spaCy faz esse c√°lculo de similaridade?

A similaridade √© determinada utilizando-se vetores de palavras, que s√£o representa√ß√µes
multi dimensionais do significado de cada palavra.

Voc√™ deve ter ouvido falar do Word2Vec, um algoritmo que √© usado com frequ√™ncia para
treinar vetores de palavras a partir de textos.

Os vetores podem ser adicionados aos modelos estat√≠sticos da spaCy.

Por padr√£o, a similaridade calculada pela spaCy √© a similaridade cosseno entre os
dois vetores, mas isso pode ser alterado se necess√°rio.

O vetor de um objeto consistido de v√°rios tokens, como o `Doc` e o `Span`, √© calculado 
como a m√©dia dos vetores dos seus tokens.

√â por este motivo que voc√™ consegue extrair mais valor de frases curtas com poucas
palavras irrelevantes.

---

# Vetores de palavras na spaCy

```python
# Carregar um fluxo (pipeline) de processamento maior com vetores
nlp = spacy.load("en_core_web_md")

doc = nlp("I have a banana")
# Acessar o vetor atrav√©s do atributo token.vector
print(doc[3].vector)
```

```out
 [2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,
  3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,
 -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,
  5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,
 -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,
  1.73800007e-01,   1.67720005e-01,   8.39839995e-01,
  5.51070012e-02,   1.05470002e-01,   3.78719985e-01,
  2.42750004e-01,   1.47449998e-02,   5.59509993e-01,
  1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,
 -4.00279984e-02,   9.59490016e-02,  -5.06900012e-01,
 -8.53179991e-02,   1.79800004e-01,   3.38669986e-01,
  ...
```

Notes: Para termos uma id√©ia de como s√£o esses vetores, vamos ver este exemplo.

Primeiro, carregamos o fluxo (pipeline) de processamento m√©dio novamente, que inclui os vetores das palavras.

Em seguida, processamos o texto e consultamos o vetor de um token atrav√©s do atributo
`.vector`.

O resultado √© um vetor com 300 dimens√µes para a palavra "banana".

---

# A similaridade depende do contexto da aplica√ß√£o 

- √ötil para diversas aplica√ß√µes: sistemas de recomenda√ß√£o, identificar textos duplicados, etc.
- N√£o h√° uma defini√ß√£o objetiva do que √© "similaridade"
- Depende do contexto e do que se espera de resultado da aplica√ß√£o.


```python
doc1 = nlp("I like cats")
doc2 = nlp("I hate cats")

print(doc1.similarity(doc2))
```

```out
0.9501447503553421
```

Notes: Prever similaridades pode ser bastante √∫til em algumas aplica√ß√µes. Por
exemplo, para recomendar para um usu√°rio textos similares aos que ele j√° leu.
Tamb√©m pode ser usado para identificar conte√∫dos duplicados, como publica√ß√µes
em plataformas online.

Contudo √© importante ter em mente que n√£o existe uma defini√ß√£o objetiva daquilo
que √© similar ou n√£o. Isso sempre vai depender do contexto e do objetivo da
sua aplica√ß√£o.

Analise este exemplo: os vetores padr√£o das palavras da biblioteca spaCy atribuem um score
de alta similaridade entre "I like cats" e "I hate cats". Isso faz sentido,
pois os dois textos expressam sentimentos relacionados a gatos. Mas no contexto
de uma aplica√ß√£o, voc√™ pode considerar que as duas frases s√£o _pouco similares_, 
pois expressam sentimentos opostos.

---

# Vamos praticar!

Notes: Agora √© a sua vez. Vamos dar uma olhada nos vetores de palavras da spaCy
e us√°-los para prever similaridades.
